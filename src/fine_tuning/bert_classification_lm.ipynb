{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "bert_classification_lm",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a13a077329274f65bf02bb35bdf116c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_077706342f6241aa90f5da71c6538bad",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_4a2dd85dd6834b6cae3f093fb68f83fe",
              "IPY_MODEL_c41693cf193544829ca951b5cb76314e"
            ]
          }
        },
        "077706342f6241aa90f5da71c6538bad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4a2dd85dd6834b6cae3f093fb68f83fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_400759dc37db419784706f2a906e6d46",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 570,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 570,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_50986d9f10f343dd90e6d9d5ebfa03df"
          }
        },
        "c41693cf193544829ca951b5cb76314e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_8d29d46dcb32428f9513b9d71b31daeb",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 570/570 [00:34&lt;00:00, 16.3B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_866225036a9e44939f50d62573322a5c"
          }
        },
        "400759dc37db419784706f2a906e6d46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "50986d9f10f343dd90e6d9d5ebfa03df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8d29d46dcb32428f9513b9d71b31daeb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "866225036a9e44939f50d62573322a5c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "db49e9748fe146a99466ce9ffb25f964": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_809a3721db9f493385a51bda2d75f81b",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_6de5e5ad52694b8a96d9a9cbfd4ba093",
              "IPY_MODEL_d97ac8a959704f8abfb4654307e64177"
            ]
          }
        },
        "809a3721db9f493385a51bda2d75f81b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6de5e5ad52694b8a96d9a9cbfd4ba093": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_862da5611690424fb5f1d0783b752a7c",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 440473133,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 440473133,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_67112f6c831c4542b5dd0487098596cf"
          }
        },
        "d97ac8a959704f8abfb4654307e64177": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_99a3f1e6a9c140e7a61491e801463da6",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 440M/440M [00:34&lt;00:00, 12.8MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e4c6a6eb7dc34371b2db0c5c85ece07a"
          }
        },
        "862da5611690424fb5f1d0783b752a7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "67112f6c831c4542b5dd0487098596cf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "99a3f1e6a9c140e7a61491e801463da6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e4c6a6eb7dc34371b2db0c5c85ece07a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5tUoHe9FRhs"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dr7BCHS-nIRW",
        "scrolled": true
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "!pip install tensorflow-gpu\n",
        "!pip install datasets transformers\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "from torch.nn import BCEWithLogitsLoss, BCELoss\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.metrics import classification_report, confusion_matrix, multilabel_confusion_matrix, f1_score, accuracy_score\n",
        "import pickle\n",
        "!pip install sentencepiece\n",
        "import transformers\n",
        "from tqdm import tqdm, trange\n",
        "from ast import literal_eval\n",
        "import ast\n",
        "from google.colab import output\n",
        "output.clear()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zB5Dij6JuItX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "220c77ed-e719-43ff-e736-9a23c83a6ee3"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uorMX_zrnISM",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "67659527-ad0b-43aa-fcd3-c56a90f610d7"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "torch.cuda.get_device_name(0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Tesla T4'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLcetMjZFjSH"
      },
      "source": [
        "## Load and Preprocess Training Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rieCYyNXgZ6"
      },
      "source": [
        "load_path = '/content/drive/MyDrive/NLP_Project/data/sentence_broken/'\n",
        "sentence_df = pd.read_pickle(load_path + 'sentence_df.pkl')\n",
        "sentence_level_df = pd.read_pickle(load_path + 'sentence_level_df.pkl')\n",
        "\n",
        "sentence_df = sentence_df.reset_index(drop=True)\n",
        "sentence_level_df = sentence_level_df.reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E57KieQbZggL",
        "outputId": "8e74f2bd-fa7d-4d20-917f-191b4dfdbd91"
      },
      "source": [
        "tags = sentence_level_df['Genre'].tolist()\n",
        "tags"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Action',\n",
              " 'Adventure',\n",
              " 'Comedy',\n",
              " 'Drama',\n",
              " 'Fantasy',\n",
              " 'Romance',\n",
              " 'School Life',\n",
              " 'Sci Fi',\n",
              " 'Shoujo',\n",
              " 'Shounen',\n",
              " 'Supernatural',\n",
              " 'Yaoi']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jqvf6GiZl1D"
      },
      "source": [
        "sentence_df['Tags'] = sentence_df['Tags'].apply(ast.literal_eval)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-XQn9KDbYHH"
      },
      "source": [
        "def remove_tags(genre):\n",
        "  selected = [ g for g in genre if g in tags ]\n",
        "  return selected"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJKBAkojZp4u"
      },
      "source": [
        "sentence_df['Tags'] = sentence_df['Tags'].apply(remove_tags)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W3D3FPOVd3en",
        "outputId": "79fb7b95-5bbb-4e83-86f8-83fc9d26bae9"
      },
      "source": [
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "mlb = MultiLabelBinarizer()\n",
        "mlb.fit(sentence_df['Tags'])\n",
        "mlb.transform(sentence_df['Tags'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 0, 0, ..., 1, 0, 0],\n",
              "       [1, 1, 0, ..., 1, 0, 0],\n",
              "       [1, 0, 0, ..., 1, 0, 0],\n",
              "       ...,\n",
              "       [1, 1, 1, ..., 0, 0, 0],\n",
              "       [1, 1, 1, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVpexR58d8b4"
      },
      "source": [
        "y = pd.DataFrame(mlb.fit_transform(sentence_df['Tags']), columns=mlb.classes_, index=sentence_df.index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "sOr-N-uLeLdS",
        "outputId": "0eba2e7b-9da7-4267-a4a7-af1d778c6606"
      },
      "source": [
        "y"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Action</th>\n",
              "      <th>Adventure</th>\n",
              "      <th>Comedy</th>\n",
              "      <th>Drama</th>\n",
              "      <th>Fantasy</th>\n",
              "      <th>Romance</th>\n",
              "      <th>School Life</th>\n",
              "      <th>Sci Fi</th>\n",
              "      <th>Shoujo</th>\n",
              "      <th>Shounen</th>\n",
              "      <th>Supernatural</th>\n",
              "      <th>Yaoi</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36810</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36811</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36812</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36813</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36814</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>36815 rows Ã— 12 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       Action  Adventure  Comedy  Drama  ...  Shoujo  Shounen  Supernatural  Yaoi\n",
              "0           1          0       0      1  ...       0        1             0     0\n",
              "1           1          1       0      1  ...       0        1             0     0\n",
              "2           1          0       0      0  ...       0        1             0     0\n",
              "3           0          0       0      0  ...       0        1             0     0\n",
              "4           0          0       0      1  ...       0        1             0     0\n",
              "...       ...        ...     ...    ...  ...     ...      ...           ...   ...\n",
              "36810       0          0       0      1  ...       1        0             1     0\n",
              "36811       1          0       0      0  ...       0        1             1     0\n",
              "36812       1          1       1      0  ...       0        0             0     0\n",
              "36813       1          1       1      0  ...       0        0             0     0\n",
              "36814       0          0       0      1  ...       0        0             0     0\n",
              "\n",
              "[36815 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufDxbCnSeJfn"
      },
      "source": [
        "df = pd.concat([sentence_df['Description'], y], axis = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 606
        },
        "id": "Gv5IFXMUeltP",
        "outputId": "f8f05c5a-a904-4ddf-be9d-c6c6f0f87aaf"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Description</th>\n",
              "      <th>Action</th>\n",
              "      <th>Adventure</th>\n",
              "      <th>Comedy</th>\n",
              "      <th>Drama</th>\n",
              "      <th>Fantasy</th>\n",
              "      <th>Romance</th>\n",
              "      <th>School Life</th>\n",
              "      <th>Sci Fi</th>\n",
              "      <th>Shoujo</th>\n",
              "      <th>Shounen</th>\n",
              "      <th>Supernatural</th>\n",
              "      <th>Yaoi</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[four year since scout regiment reached shorel...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[foundation alchemy based law equivalent excha...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[battle retake wall maria begin, eren new hard...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[picking second season ended boy prepare final...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[transferring new school deaf girl shouko nish...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36810</th>\n",
              "      <td>[natsume meeting new youkai helping, lot frien...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36811</th>\n",
              "      <td>[zombie loan zombie loan special girl name mic...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36812</th>\n",
              "      <td>[NUM year old james lynx officer lev pilot uni...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36813</th>\n",
              "      <td>[NUM year old james lynx officer lev pilot uni...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36814</th>\n",
              "      <td>[late NUM china xiaoyi high school student tak...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>36815 rows Ã— 13 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             Description  ...  Yaoi\n",
              "0      [four year since scout regiment reached shorel...  ...     0\n",
              "1      [foundation alchemy based law equivalent excha...  ...     0\n",
              "2      [battle retake wall maria begin, eren new hard...  ...     0\n",
              "3      [picking second season ended boy prepare final...  ...     0\n",
              "4      [transferring new school deaf girl shouko nish...  ...     0\n",
              "...                                                  ...  ...   ...\n",
              "36810  [natsume meeting new youkai helping, lot frien...  ...     0\n",
              "36811  [zombie loan zombie loan special girl name mic...  ...     0\n",
              "36812  [NUM year old james lynx officer lev pilot uni...  ...     0\n",
              "36813  [NUM year old james lynx officer lev pilot uni...  ...     0\n",
              "36814  [late NUM china xiaoyi high school student tak...  ...     0\n",
              "\n",
              "[36815 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "jYdraZC_emuJ",
        "outputId": "f8de6edc-9270-46be-8d48-7f7a7f0bdf16"
      },
      "source": [
        "sentence_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Title</th>\n",
              "      <th>Description</th>\n",
              "      <th>Tags</th>\n",
              "      <th>Length</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Attack on Titan The Final Season</td>\n",
              "      <td>[four year since scout regiment reached shorel...</td>\n",
              "      <td>[Action, Drama, Fantasy, Shounen]</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Fullmetal Alchemist: Brotherhood</td>\n",
              "      <td>[foundation alchemy based law equivalent excha...</td>\n",
              "      <td>[Action, Adventure, Drama, Fantasy, Shounen]</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Attack on Titan 3rd Season: Part II</td>\n",
              "      <td>[battle retake wall maria begin, eren new hard...</td>\n",
              "      <td>[Action, Fantasy, Shounen]</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Haikyuu!! Karasuno High School vs Shiratorizaw...</td>\n",
              "      <td>[picking second season ended boy prepare final...</td>\n",
              "      <td>[Shounen, School Life]</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>A Silent Voice</td>\n",
              "      <td>[transferring new school deaf girl shouko nish...</td>\n",
              "      <td>[Drama, Shounen, School Life]</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36810</th>\n",
              "      <td>Zoku Natsume Yuujinchou</td>\n",
              "      <td>[natsume meeting new youkai helping, lot frien...</td>\n",
              "      <td>[Drama, Fantasy, Shoujo, Supernatural]</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36811</th>\n",
              "      <td>Zombie Loan</td>\n",
              "      <td>[zombie loan zombie loan special girl name mic...</td>\n",
              "      <td>[Action, Shounen, Supernatural]</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36812</th>\n",
              "      <td>Zone of the Enders: Dolores (Dub)</td>\n",
              "      <td>[NUM year old james lynx officer lev pilot uni...</td>\n",
              "      <td>[Action, Adventure, Comedy]</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36813</th>\n",
              "      <td>Zone of the Enders: Dolores, I</td>\n",
              "      <td>[NUM year old james lynx officer lev pilot uni...</td>\n",
              "      <td>[Action, Adventure, Comedy]</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36814</th>\n",
              "      <td>Zuori Qing Kong</td>\n",
              "      <td>[late NUM china xiaoyi high school student tak...</td>\n",
              "      <td>[Drama, Romance]</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>36815 rows Ã— 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   Title  ... Length\n",
              "0                       Attack on Titan The Final Season  ...      4\n",
              "1                       Fullmetal Alchemist: Brotherhood  ...      7\n",
              "2                    Attack on Titan 3rd Season: Part II  ...      5\n",
              "3      Haikyuu!! Karasuno High School vs Shiratorizaw...  ...      3\n",
              "4                                         A Silent Voice  ...      4\n",
              "...                                                  ...  ...    ...\n",
              "36810                            Zoku Natsume Yuujinchou  ...      3\n",
              "36811                                        Zombie Loan  ...      5\n",
              "36812                  Zone of the Enders: Dolores (Dub)  ...      4\n",
              "36813                     Zone of the Enders: Dolores, I  ...      4\n",
              "36814                                    Zuori Qing Kong  ...      3\n",
              "\n",
              "[36815 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVI59S9VaAfB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4718c9ae-a553-4262-d541-9ce69090f7c6"
      },
      "source": [
        "cols = df.columns\n",
        "label_cols = list(cols[1:])\n",
        "num_labels = len(label_cols)\n",
        "print('Label columns: ', label_cols)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Label columns:  ['Action', 'Adventure', 'Comedy', 'Drama', 'Fantasy', 'Romance', 'School Life', 'Sci Fi', 'Shoujo', 'Shounen', 'Supernatural', 'Yaoi']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzgA5qQgYIBZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8da352b7-f501-455c-8c9b-5f47aa6945c3"
      },
      "source": [
        "print('Count of 1 per label: \\n', df[label_cols].sum(), '\\n') # Label counts, may need to downsample or upsample\n",
        "print('Count of 0 per label: \\n', df[label_cols].eq(0).sum())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Count of 1 per label: \n",
            " Action           8560\n",
            "Adventure        4748\n",
            "Comedy          11365\n",
            "Drama           10925\n",
            "Fantasy          8088\n",
            "Romance         15736\n",
            "School Life      6285\n",
            "Sci Fi           2874\n",
            "Shoujo           4835\n",
            "Shounen          5046\n",
            "Supernatural     4712\n",
            "Yaoi             5052\n",
            "dtype: int64 \n",
            "\n",
            "Count of 0 per label: \n",
            " Action          28255\n",
            "Adventure       32067\n",
            "Comedy          25450\n",
            "Drama           25890\n",
            "Fantasy         28727\n",
            "Romance         21079\n",
            "School Life     30530\n",
            "Sci Fi          33941\n",
            "Shoujo          31980\n",
            "Shounen         31769\n",
            "Supernatural    32103\n",
            "Yaoi            31763\n",
            "dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFpSd4JzaAae"
      },
      "source": [
        "df = df.sample(frac=1).reset_index(drop=True) #shuffle rows"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DF3ddjej5vd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "b2d3090e-b634-4ea6-b92a-57446fdb4d1b"
      },
      "source": [
        "df['one_hot_labels'] = list(df[label_cols].values)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Description</th>\n",
              "      <th>Action</th>\n",
              "      <th>Adventure</th>\n",
              "      <th>Comedy</th>\n",
              "      <th>Drama</th>\n",
              "      <th>Fantasy</th>\n",
              "      <th>Romance</th>\n",
              "      <th>School Life</th>\n",
              "      <th>Sci Fi</th>\n",
              "      <th>Shoujo</th>\n",
              "      <th>Shounen</th>\n",
              "      <th>Supernatural</th>\n",
              "      <th>Yaoi</th>\n",
              "      <th>one_hot_labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[middle schoolers long distance love, happens ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[staff village etrangers welcome suave new man...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[tall handsome wealthy NUM year old predecesso...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[two played secret game kid, yasuhiro always t...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[eunkyum company year, friendly bright great r...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                         Description  ...                        one_hot_labels\n",
              "0  [middle schoolers long distance love, happens ...  ...  [0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0]\n",
              "1  [staff village etrangers welcome suave new man...  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
              "2  [tall handsome wealthy NUM year old predecesso...  ...  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
              "3  [two played secret game kid, yasuhiro always t...  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
              "4  [eunkyum company year, friendly bright great r...  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
              "\n",
              "[5 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ks0Zal0qgs9f"
      },
      "source": [
        "df['Description'] = df['Description'].str.join(\".\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JlyRGCPUsuIx"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_df, test_df = train_test_split(df, test_size=0.1)\n",
        "train_df = train_df.reset_index(drop=True)\n",
        "test_df = test_df.reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlhHifh5bW7e"
      },
      "source": [
        "labels = list(train_df.one_hot_labels.values)\n",
        "comments = list(train_df.Description.values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhVr8SziL_PY"
      },
      "source": [
        "In order to avoid memory issues with Google Colab, I enforce a max_length of 100 tokens. Note that some sentences may not adequately represent each label because of this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqB8TwQOtqA9"
      },
      "source": [
        "# model_checkpoint = \"distilroberta-base-uncased\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNsEu-vUur-4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71282e05-1039-4934-ae0f-f9d4abfe4c19"
      },
      "source": [
        "from transformers import AutoTokenizer, BertTokenizer\n",
        "max_length = 100\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True) # tokenizer\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, do_lower_case=True) # tokenizer\n",
        "encodings = tokenizer.batch_encode_plus(comments,max_length=max_length,pad_to_max_length=True) # tokenizer's encoding method\n",
        "print('tokenizer outputs: ', encodings.keys())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2111: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tokenizer outputs:  dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6CCLSjfur-9"
      },
      "source": [
        "input_ids = encodings['input_ids'] # tokenized and encoded sentences\n",
        "token_type_ids = encodings['token_type_ids'] # token type ids\n",
        "attention_masks = encodings['attention_mask'] # attention masks"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSOFbThlYcpb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7dc7975d-c863-4c84-82fc-c9fd9531c487"
      },
      "source": [
        "# Identifying indices of 'one_hot_labels' entries that only occur once - this will allow us to stratify split our training data later\n",
        "label_counts = train_df.one_hot_labels.astype(str).value_counts()\n",
        "one_freq = label_counts[label_counts==1].keys()\n",
        "one_freq_idxs = sorted(list(train_df[train_df.one_hot_labels.astype(str).isin(one_freq)].index), reverse=True)\n",
        "print('train_df label indices with only one instance: ', one_freq_idxs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train_df label indices with only one instance:  [33000, 32998, 32866, 32798, 32720, 32390, 32346, 32180, 31897, 31884, 31809, 31764, 31756, 31737, 31697, 31622, 31393, 31372, 31057, 31011, 30967, 30833, 30629, 30491, 30444, 30289, 30163, 30076, 29831, 29760, 29751, 29740, 29720, 29229, 29057, 29020, 28907, 28832, 28684, 28568, 28330, 28248, 28240, 28224, 28194, 28136, 28086, 28023, 27901, 27826, 27809, 27736, 27575, 27406, 27258, 27199, 26879, 26767, 26234, 26207, 26102, 25993, 25947, 25726, 25460, 25354, 25017, 24996, 24360, 24256, 24232, 24225, 24147, 24099, 23921, 23895, 23406, 23396, 23122, 23044, 22687, 22645, 22636, 22630, 22628, 22508, 22444, 22413, 22322, 22007, 21959, 21902, 21779, 21734, 21632, 21384, 21104, 20680, 20200, 20171, 19943, 19766, 19635, 19426, 19286, 19173, 18884, 18883, 18838, 18691, 18549, 18537, 18374, 18359, 18278, 18215, 18205, 17813, 17590, 17537, 17385, 17283, 17044, 17026, 16989, 16924, 16596, 16427, 16194, 16129, 16046, 15777, 15742, 15696, 15515, 15508, 15498, 15349, 15295, 15207, 14932, 14835, 14664, 14479, 14462, 14429, 14355, 14179, 14127, 13867, 13815, 13645, 13565, 13486, 13247, 12907, 12801, 12741, 12602, 12399, 12378, 12119, 11956, 11939, 11662, 11639, 11606, 11505, 11382, 11351, 11306, 11304, 11001, 10869, 10706, 10360, 10354, 10329, 10299, 10288, 10201, 9800, 9742, 9221, 9142, 8914, 8905, 8727, 8682, 8537, 8461, 8323, 8299, 8298, 8096, 8005, 7881, 7630, 7617, 7563, 7133, 6933, 6853, 6682, 6526, 6442, 6393, 6239, 6047, 6008, 5985, 5930, 5791, 5737, 5683, 5619, 5545, 5422, 5277, 4664, 4539, 4445, 4351, 4336, 4332, 4255, 4196, 4179, 4113, 4070, 3921, 3874, 3789, 3734, 3667, 3660, 3515, 3475, 3397, 3283, 3274, 3190, 3152, 3079, 3041, 3003, 2976, 2874, 2738, 2634, 2529, 2508, 2480, 1933, 1411, 1380, 1286, 1276, 1219, 1063, 1024, 764, 710, 648, 368, 249, 217, 188]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQQ7CoOag_r7"
      },
      "source": [
        "# Gathering single instance inputs to force into the training set after stratified split\n",
        "one_freq_input_ids = [input_ids.pop(i) for i in one_freq_idxs]\n",
        "one_freq_token_types = [token_type_ids.pop(i) for i in one_freq_idxs]\n",
        "one_freq_attention_masks = [attention_masks.pop(i) for i in one_freq_idxs]\n",
        "one_freq_labels = [labels.pop(i) for i in one_freq_idxs]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9PxAt48HRRj"
      },
      "source": [
        "Be sure to handle all classes during validation using \"stratify\" during train/validation split:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPFaq4ufnIT2"
      },
      "source": [
        "# Use train_test_split to split our data into train and validation sets\n",
        "train_inputs, validation_inputs, train_labels, validation_labels, train_token_types, validation_token_types, train_masks, validation_masks = train_test_split(input_ids, labels, token_type_ids, attention_masks,\n",
        "                                                            random_state=2020, test_size=0.10, stratify = labels)\n",
        "# Add one frequency data to train data\n",
        "train_inputs.extend(one_freq_input_ids)\n",
        "train_labels.extend(one_freq_labels)\n",
        "train_masks.extend(one_freq_attention_masks)\n",
        "train_token_types.extend(one_freq_token_types)\n",
        "\n",
        "# Convert all of our data into torch tensors, the required datatype for our model\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "train_labels = torch.tensor(train_labels)\n",
        "train_masks = torch.tensor(train_masks)\n",
        "train_token_types = torch.tensor(train_token_types)\n",
        "\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "validation_labels = torch.tensor(validation_labels)\n",
        "validation_masks = torch.tensor(validation_masks)\n",
        "validation_token_types = torch.tensor(validation_token_types)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRnuLna-nIT4"
      },
      "source": [
        "# Select a batch size for training. For fine-tuning with XLNet, the authors recommend a batch size of 32, 48, or 128. We will use 32 here to avoid memory issues.\n",
        "batch_size = 32\n",
        "\n",
        "# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n",
        "# with an iterator the entire dataset does not need to be loaded into memory\n",
        "\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels, train_token_types)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels, validation_token_types)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiFRnP_ZTBFa"
      },
      "source": [
        "torch.save(validation_dataloader,'validation_data_loader')\n",
        "torch.save(train_dataloader,'train_data_loader')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncGteBuSFuZM"
      },
      "source": [
        "## Load Model & SetÂ Params"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ujk4k16DnIT6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "a13a077329274f65bf02bb35bdf116c5",
            "077706342f6241aa90f5da71c6538bad",
            "4a2dd85dd6834b6cae3f093fb68f83fe",
            "c41693cf193544829ca951b5cb76314e",
            "400759dc37db419784706f2a906e6d46",
            "50986d9f10f343dd90e6d9d5ebfa03df",
            "8d29d46dcb32428f9513b9d71b31daeb",
            "866225036a9e44939f50d62573322a5c",
            "db49e9748fe146a99466ce9ffb25f964",
            "809a3721db9f493385a51bda2d75f81b",
            "6de5e5ad52694b8a96d9a9cbfd4ba093",
            "d97ac8a959704f8abfb4654307e64177",
            "862da5611690424fb5f1d0783b752a7c",
            "67112f6c831c4542b5dd0487098596cf",
            "99a3f1e6a9c140e7a61491e801463da6",
            "e4c6a6eb7dc34371b2db0c5c85ece07a"
          ]
        },
        "outputId": "707f9790-cede-4e4c-e5ca-8fa7ee1f6c00"
      },
      "source": [
        "# Load model, the pretrained model will include a single linear classification layer on top for classification. \n",
        "from transformers import AutoModelForSequenceClassification, BertForSequenceClassification\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=num_labels)\n",
        "# model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)\n",
        "model.cuda()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a13a077329274f65bf02bb35bdf116c5",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=570.0, style=ProgressStyle(description_â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "db49e9748fe146a99466ce9ffb25f964",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descriâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=12, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGE4gv9qfhRG"
      },
      "source": [
        "Setting custom optimization parameters for the AdamW optimizer https://huggingface.co/transformers/main_classes/optimizer_schedules.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GsV8zwWYnIT9"
      },
      "source": [
        "# setting custom optimization parameters. You may implement a scheduler here as well.\n",
        "param_optimizer = list(model.named_parameters())\n",
        "no_decay = ['bias', 'gamma', 'beta']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.01},\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.0}\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOomZIEIoHOL"
      },
      "source": [
        "from transformers import AdamW\n",
        "optimizer = AdamW(optimizer_grouped_parameters,lr=2e-5,correct_bias=True)\n",
        "# optimizer = AdamW(model.parameters(),lr=2e-5)  # Default optimization"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRQQZ8zIFzLW"
      },
      "source": [
        "## Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDLZmEC_oKo3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "491b5332-2041-4bf1-8c2e-b9660c179b4b"
      },
      "source": [
        "# Store our loss and accuracy for plotting\n",
        "train_loss_set = []\n",
        "\n",
        "# Number of training epochs (authors recommend between 2 and 4)\n",
        "epochs = 4\n",
        "\n",
        "# trange is a tqdm wrapper around the normal python range\n",
        "for _ in trange(epochs, desc=\"Epoch\"):\n",
        "\n",
        "  # Training\n",
        "  \n",
        "  # Set our model to training mode (as opposed to evaluation mode)\n",
        "  model.train()\n",
        "\n",
        "  # Tracking variables\n",
        "  tr_loss = 0 #running loss\n",
        "  nb_tr_examples, nb_tr_steps = 0, 0\n",
        "  \n",
        "  # Train the data for one epoch\n",
        "  for step, batch in enumerate(train_dataloader):\n",
        "    # Add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    # Unpack the inputs from our dataloader\n",
        "    b_input_ids, b_input_mask, b_labels, b_token_types = batch\n",
        "    # Clear out the gradients (by default they accumulate)\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # # Forward pass for multiclass classification\n",
        "    # outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
        "    # loss = outputs[0]\n",
        "    # logits = outputs[1]\n",
        "\n",
        "    # Forward pass for multilabel classification\n",
        "    outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "    logits = outputs[0]\n",
        "    loss_func = BCEWithLogitsLoss() \n",
        "    loss = loss_func(logits.view(-1,num_labels),b_labels.type_as(logits).view(-1,num_labels)) #convert labels to float for calculation\n",
        "    # loss_func = BCELoss() \n",
        "    # loss = loss_func(torch.sigmoid(logits.view(-1,num_labels)),b_labels.type_as(logits).view(-1,num_labels)) #convert labels to float for calculation\n",
        "    train_loss_set.append(loss.item())    \n",
        "\n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "    # Update parameters and take a step using the computed gradient\n",
        "    optimizer.step()\n",
        "    # scheduler.step()\n",
        "    # Update tracking variables\n",
        "    tr_loss += loss.item()\n",
        "    nb_tr_examples += b_input_ids.size(0)\n",
        "    nb_tr_steps += 1\n",
        "\n",
        "  print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
        "\n",
        "###############################################################################\n",
        "\n",
        "  # Validation\n",
        "\n",
        "  # Put model in evaluation mode to evaluate loss on the validation set\n",
        "  model.eval()\n",
        "\n",
        "  # Variables to gather full output\n",
        "  logit_preds,true_labels,pred_labels,tokenized_texts = [],[],[],[]\n",
        "\n",
        "  # Predict\n",
        "  for i, batch in enumerate(validation_dataloader):\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    # Unpack the inputs from our dataloader\n",
        "    b_input_ids, b_input_mask, b_labels, b_token_types = batch\n",
        "    with torch.no_grad():\n",
        "      # Forward pass\n",
        "      outs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "      b_logit_pred = outs[0]\n",
        "      pred_label = torch.sigmoid(b_logit_pred)\n",
        "\n",
        "      b_logit_pred = b_logit_pred.detach().cpu().numpy()\n",
        "      pred_label = pred_label.to('cpu').numpy()\n",
        "      b_labels = b_labels.to('cpu').numpy()\n",
        "\n",
        "    tokenized_texts.append(b_input_ids)\n",
        "    logit_preds.append(b_logit_pred)\n",
        "    true_labels.append(b_labels)\n",
        "    pred_labels.append(pred_label)\n",
        "\n",
        "  # Flatten outputs\n",
        "  pred_labels = [item for sublist in pred_labels for item in sublist]\n",
        "  true_labels = [item for sublist in true_labels for item in sublist]\n",
        "\n",
        "  # Calculate Accuracy\n",
        "  threshold = 0.50\n",
        "  pred_bools = [pl>threshold for pl in pred_labels]\n",
        "  true_bools = [tl==1 for tl in true_labels]\n",
        "  val_f1_accuracy = f1_score(true_bools,pred_bools,average='micro')*100\n",
        "  val_flat_accuracy = accuracy_score(true_bools, pred_bools)*100\n",
        "\n",
        "  print('F1 Validation Accuracy: ', val_f1_accuracy)\n",
        "  print('Flat Validation Accuracy: ', val_flat_accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:   0%|          | 0/4 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 0.3845348054062004\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [08:30<25:31, 510.38s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "F1 Validation Accuracy:  50.4163890739507\n",
            "Flat Validation Accuracy:  16.215393976270153\n",
            "Train loss: 0.3243265437862625\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [17:12<17:07, 513.90s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "F1 Validation Accuracy:  57.47195213163799\n",
            "Flat Validation Accuracy:  18.132035290538482\n",
            "Train loss: 0.2892650183757402\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [25:55<08:36, 516.56s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "F1 Validation Accuracy:  58.84433128226891\n",
            "Flat Validation Accuracy:  18.071189534529967\n",
            "Train loss: 0.2540067004045873\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [34:37<00:00, 519.41s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "F1 Validation Accuracy:  58.67243867243867\n",
            "Flat Validation Accuracy:  18.527532704593856\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aiBeiBSRoOuz"
      },
      "source": [
        "torch.save(model.state_dict(), 'bert_classification_lm')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjJkFqkligmD"
      },
      "source": [
        "model.save_pretrained('/content/LM')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7pFL63sIihA7"
      },
      "source": [
        "!mkdir -p /content/drive/MyDrive/Final_NLP_Project/Part5/bert_classification_lm\n",
        "!cp -a /content/LM/. /content/drive/MyDrive/Final_NLP_Project/Part5/bert_classification_lm\n",
        "!cp  /content/bert_classification_lm /content/drive/MyDrive/Final_NLP_Project/Part5/bert_classification_lm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7dd2GE3F4yK"
      },
      "source": [
        "## Load and Preprocess Test Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1a41OmU2i7qp"
      },
      "source": [
        "# Gathering input data\n",
        "test_labels = list(test_df.one_hot_labels.values)\n",
        "test_comments = list(test_df.Description.values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amySMO8EQzf2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "158fc12a-1273-47a8-d50e-ed11dd2a4924"
      },
      "source": [
        "# Encoding input data\n",
        "test_encodings = tokenizer.batch_encode_plus(test_comments,max_length=max_length,pad_to_max_length=True)\n",
        "test_input_ids = test_encodings['input_ids']\n",
        "test_token_type_ids = test_encodings['token_type_ids']\n",
        "test_attention_masks = test_encodings['attention_mask']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2111: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqOfi9fkRaRN"
      },
      "source": [
        "# Make tensors out of data\n",
        "test_inputs = torch.tensor(test_input_ids)\n",
        "test_labels = torch.tensor(test_labels)\n",
        "test_masks = torch.tensor(test_attention_masks)\n",
        "test_token_types = torch.tensor(test_token_type_ids)\n",
        "# Create test dataloader\n",
        "test_data = TensorDataset(test_inputs, test_masks, test_labels, test_token_types)\n",
        "test_sampler = SequentialSampler(test_data)\n",
        "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n",
        "# Save test dataloader\n",
        "torch.save(test_dataloader,'test_data_loader')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFTWxCA_GBau"
      },
      "source": [
        "## Prediction and Metics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPvrL6OFSQvf"
      },
      "source": [
        "# Test\n",
        "\n",
        "# Put model in evaluation mode to evaluate loss on the validation set\n",
        "model.eval()\n",
        "\n",
        "#track variables\n",
        "logit_preds,true_labels,pred_labels,tokenized_texts = [],[],[],[]\n",
        "\n",
        "# Predict\n",
        "for i, batch in enumerate(test_dataloader):\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  # Unpack the inputs from our dataloader\n",
        "  b_input_ids, b_input_mask, b_labels, b_token_types = batch\n",
        "  with torch.no_grad():\n",
        "    # Forward pass\n",
        "    outs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "    b_logit_pred = outs[0]\n",
        "    pred_label = torch.sigmoid(b_logit_pred)\n",
        "\n",
        "    b_logit_pred = b_logit_pred.detach().cpu().numpy()\n",
        "    pred_label = pred_label.to('cpu').numpy()\n",
        "    b_labels = b_labels.to('cpu').numpy()\n",
        "\n",
        "  tokenized_texts.append(b_input_ids)\n",
        "  logit_preds.append(b_logit_pred)\n",
        "  true_labels.append(b_labels)\n",
        "  pred_labels.append(pred_label)\n",
        "\n",
        "# Flatten outputs\n",
        "tokenized_texts = [item for sublist in tokenized_texts for item in sublist]\n",
        "pred_labels = [item for sublist in pred_labels for item in sublist]\n",
        "true_labels = [item for sublist in true_labels for item in sublist]\n",
        "# Converting flattened binary values to boolean values\n",
        "true_bools = [tl==1 for tl in true_labels]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQeGWqeMzAoZ"
      },
      "source": [
        "We need to threshold our sigmoid function outputs which range from [0, 1]. Below I use 0.50 as a threshold."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZcZUcYOxxmM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "626917dd-922f-446a-8b80-bc857ed81a37"
      },
      "source": [
        "pred_bools = [pl>0.50 for pl in pred_labels] #boolean output after thresholding\n",
        "test_label_cols = list(test_df.columns[1:13])\n",
        "print(test_label_cols)\n",
        "# Print and save classification report\n",
        "print('Test F1 Accuracy: ', f1_score(true_bools, pred_bools,average='micro'))\n",
        "print('Test Flat Accuracy: ', accuracy_score(true_bools, pred_bools),'\\n')\n",
        "clf_report = classification_report(true_bools,pred_bools,target_names=test_label_cols)\n",
        "pickle.dump(clf_report, open('classification_report.txt','wb')) #save report\n",
        "print(clf_report)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Action', 'Adventure', 'Comedy', 'Drama', 'Fantasy', 'Romance', 'School Life', 'Sci Fi', 'Shoujo', 'Shounen', 'Supernatural', 'Yaoi']\n",
            "Test F1 Accuracy:  0.5885920030320257\n",
            "Test Flat Accuracy:  0.18902770233568714 \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Action       0.67      0.65      0.66       873\n",
            "   Adventure       0.62      0.42      0.50       462\n",
            "      Comedy       0.65      0.48      0.55      1145\n",
            "       Drama       0.55      0.40      0.46      1111\n",
            "     Fantasy       0.72      0.65      0.68       819\n",
            "     Romance       0.74      0.72      0.73      1560\n",
            " School Life       0.71      0.46      0.56       657\n",
            "      Sci Fi       0.61      0.39      0.47       312\n",
            "      Shoujo       0.59      0.45      0.51       476\n",
            "     Shounen       0.68      0.18      0.29       489\n",
            "Supernatural       0.66      0.38      0.48       468\n",
            "        Yaoi       0.69      0.69      0.69       505\n",
            "\n",
            "   micro avg       0.67      0.52      0.59      8877\n",
            "   macro avg       0.66      0.49      0.55      8877\n",
            "weighted avg       0.66      0.52      0.58      8877\n",
            " samples avg       0.64      0.54      0.55      8877\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rLqrHK87eir"
      },
      "source": [
        "## Output Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZh62L7P4hvX"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import re\n",
        "import csv\n",
        "import matplotlib.pyplot as plt \n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import ast\n",
        "import os\n",
        "from os import path\n",
        "from pandas.plotting import table\n",
        "import nltk\n",
        "nltk.download(\"popular\")\n",
        "from google.colab import output\n",
        "from google.colab import files\n",
        "import string\n",
        "output.clear()\n",
        "\n",
        "%matplotlib inline\n",
        "pd.set_option('display.max_colwidth', 300)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJBkRdGN1hzx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc492c32-1d39-49e0-aa32-a61b607be8fb"
      },
      "source": [
        "idx2label = dict(zip(range(12),label_cols))\n",
        "print(idx2label)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{0: 'Action', 1: 'Adventure', 2: 'Comedy', 3: 'Drama', 4: 'Fantasy', 5: 'Romance', 6: 'School Life', 7: 'Sci Fi', 8: 'Shoujo', 9: 'Shounen', 10: 'Supernatural', 11: 'Yaoi'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZUglV_A4BF_"
      },
      "source": [
        "# Getting indices of where boolean one hot vector true_bools is True so we can use idx2label to gather label names\n",
        "true_label_idxs, pred_label_idxs=[],[]\n",
        "for vals in true_bools:\n",
        "  true_label_idxs.append(np.where(vals)[0].flatten().tolist())\n",
        "for vals in pred_bools:\n",
        "  pred_label_idxs.append(np.where(vals)[0].flatten().tolist())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOGhXM3R4a91"
      },
      "source": [
        "# Gathering vectors of label names using idx2label\n",
        "true_label_texts, pred_label_texts = [], []\n",
        "for vals in true_label_idxs:\n",
        "  if vals:\n",
        "    true_label_texts.append([idx2label[val] for val in vals])\n",
        "  else:\n",
        "    true_label_texts.append(vals)\n",
        "\n",
        "for vals in pred_label_idxs:\n",
        "  if vals:\n",
        "    pred_label_texts.append([idx2label[val] for val in vals])\n",
        "  else:\n",
        "    pred_label_texts.append(vals)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HaqV6pn_HCG"
      },
      "source": [
        "# Decoding input ids to comment text\n",
        "comment_texts = [tokenizer.decode(text,skip_special_tokens=True,clean_up_tokenization_spaces=False) for text in tokenized_texts]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7kk0Mgl1L-T",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 975
        },
        "outputId": "2db4cedc-3832-4e2f-b65e-80ec37b44540"
      },
      "source": [
        "# Converting lists to df\n",
        "comparisons_df = pd.DataFrame({'comment_text': comment_texts, 'true_labels': true_label_texts, 'pred_labels':pred_label_texts})\n",
        "comparisons_df.to_csv('comparisons.csv')\n",
        "comparisons_df.head(20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>comment_text</th>\n",
              "      <th>true_labels</th>\n",
              "      <th>pred_labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>merry server desperate need money . one day blue peter one customer asked pretend girlfriend thanksgiving holiday . seemed serious money would certainly help merry accepted proposal . went visit family sweet mother welcomed merry open arm . believed merry son going get married . merry concerned ...</td>\n",
              "      <td>[Romance]</td>\n",
              "      <td>[Romance]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>sugita handsome skilled newcomer company get along everyone everyone except petite assistant manager yumihara tend strict . one day lunch break sugita find small secluded coffee shop named dead copy feature special table dedicated playing retro game . surprise also spot yumihara completely immer...</td>\n",
              "      <td>[Comedy, Romance]</td>\n",
              "      <td>[Yaoi]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>kashiwagi koichi recently moved cousin house father barely talked recent year pass away . cousin four female sibling ranging age adult young child chizuru azusa kaede hatsune . parent died year ago koichi father girl uncle became guardian . koichi fit perfectly household although kaede seems act...</td>\n",
              "      <td>[Drama]</td>\n",
              "      <td>[Supernatural]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>man love yeena yoon world gunho yoo . girl scared gunho yoo world yeena yoon . love letter meant brother juhyung yoo end hand scary gunho . could even correct situation reply accept confession</td>\n",
              "      <td>[Drama, Romance]</td>\n",
              "      <td>[Romance, Shoujo]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>girl wealthy family accidentally lost virginity . get insulted party waking next morning . swore get revenge got closer instead</td>\n",
              "      <td>[Drama, Romance]</td>\n",
              "      <td>[Drama, Romance]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>death criminal lived jail country palom entering mad game progress . battle freedom . meaning freedom . spill scum blood like butcher massacring people flavor freedom . swore use legend sword hand breaking apart shackle world</td>\n",
              "      <td>[Action, Adventure, Fantasy]</td>\n",
              "      <td>[Action, Fantasy]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>ippo ready face greatest test thus far boxer next opponent completely unlike anyone fought . cool calculating possessing intelligence rivaled ferocity brain ippo brawn . make matter worse old rival coach kamogawa past returned guide ippo genius opponent brings knowledge secret attack supposed un...</td>\n",
              "      <td>[Action, Drama, Shounen]</td>\n",
              "      <td>[Drama]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>lone mercenary traveling foreign land forced participate street brawl fighting back back gun toting man wearing pelt around shoulder . emerge attack unscathed man order become bodyguard . mercenary turned bodyguard learn surprising fact spirited fighter king near daily assassination attempt maje...</td>\n",
              "      <td>[Action, Fantasy]</td>\n",
              "      <td>[Action]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>slapstick comedy three pretty girl struggling earn living pilot iron goblin delivery vessel . computer answer back space pirate tail romantic entanglement client cause friction trio . final episode throw alien eterna race mix</td>\n",
              "      <td>[Comedy, Sci Fi]</td>\n",
              "      <td>[Comedy]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>unfair heaven strange world wing forged flight god forsaken duck chicken ostrich . teasing dream soaring sky confining earth . world everyone born god blessing anomaly . ergera outclass god abandoned whilst simultaneously saving people love</td>\n",
              "      <td>[Action, Romance, Supernatural]</td>\n",
              "      <td>[Adventure, Comedy, Fantasy]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>got see neighbor inside life chance . happening . take mind</td>\n",
              "      <td>[Drama]</td>\n",
              "      <td>[Romance]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>raven start sing people die unstable world weak government mafia escaping sin . order reduce unpunished crime committed mafia strength born agent merciless kill withought hesitation . last enforcement organization known red raven</td>\n",
              "      <td>[Action, Drama, Sci Fi, Shounen]</td>\n",
              "      <td>[Action]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>unusual restaurant located suburb japan invite take look livelihood staff . appeal sens mention cookware personified . pandra restaurant begin cutting board journey find place restaurant easy plastic cutting board amongst experienced utensil make</td>\n",
              "      <td>[Comedy, Romance]</td>\n",
              "      <td>[Comedy]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>man call hakase professor work top class . engineering . wish create perfect android want use beautiful male model hinatsu base . hakase becomes bit obsessed real hinatsu investigation . hakase perverted many strange thing learn anything everything hinatsu . hinatsu want nothing nut job wish loo...</td>\n",
              "      <td>[Comedy, Fantasy, Romance, School Life]</td>\n",
              "      <td>[Yaoi]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>sakurai new hire secret relationship bos chief akimoto . even though chief gentle suave unexpectedly pro come department still old man sakurai eye . however akimoto see right rather naive sakurai spoil point keep demanding come regret</td>\n",
              "      <td>[Comedy, Yaoi]</td>\n",
              "      <td>[Yaoi]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>valt aoi total newbie come beyblade battle one beat love game . valt set sight district tournament held hometown . first master move technique show . natural talent shine</td>\n",
              "      <td>[Action, Comedy, Shounen]</td>\n",
              "      <td>[Action, Adventure, Comedy, Shounen]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>year num tide zombie outbreak exploded everyone homeland humanity launch large scale battle endless night awaiting arrival dawn . end approaching going way . faith shattered piece buried ash raging fire . within blazing fire newborn hope arise</td>\n",
              "      <td>[Action, Supernatural]</td>\n",
              "      <td>[Action]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>whether tongue body use . long man fell love . back old alley yoruji always looking night sky . finally opened ramen shop yogoto despite name far cry away beloved star . however one late night yoruji discovers akemi posse special tongue . two begin work together order make best tasting ramen . a...</td>\n",
              "      <td>[Romance, Yaoi]</td>\n",
              "      <td>[Yaoi]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>second son family beauty naruse shouta world famous hairstylist model . constantly shower affection ichii hisashi grim pure hearted high school student life across street . due self imposed num prohibited rule shouta impatiently waiting hisashi grow . however none soon hisashi num birthday approach</td>\n",
              "      <td>[Comedy, Romance, Yaoi]</td>\n",
              "      <td>[Drama, Yaoi]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>deep sandy plain middle eastern kingdom asran japanese photojournalist makoto shinjou travel remote airbase area num document activity mercenary destroy country enemy living . among rank shin kazama japanese ace pilot tricked former best friend signing contract asran fighter squad . lost career ...</td>\n",
              "      <td>[Action, Drama, Romance, Shounen]</td>\n",
              "      <td>[Action, Drama]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                                                                                                                                   comment_text  ...                           pred_labels\n",
              "0   merry server desperate need money . one day blue peter one customer asked pretend girlfriend thanksgiving holiday . seemed serious money would certainly help merry accepted proposal . went visit family sweet mother welcomed merry open arm . believed merry son going get married . merry concerned ...  ...                             [Romance]\n",
              "1   sugita handsome skilled newcomer company get along everyone everyone except petite assistant manager yumihara tend strict . one day lunch break sugita find small secluded coffee shop named dead copy feature special table dedicated playing retro game . surprise also spot yumihara completely immer...  ...                                [Yaoi]\n",
              "2   kashiwagi koichi recently moved cousin house father barely talked recent year pass away . cousin four female sibling ranging age adult young child chizuru azusa kaede hatsune . parent died year ago koichi father girl uncle became guardian . koichi fit perfectly household although kaede seems act...  ...                        [Supernatural]\n",
              "3                                                                                                              man love yeena yoon world gunho yoo . girl scared gunho yoo world yeena yoon . love letter meant brother juhyung yoo end hand scary gunho . could even correct situation reply accept confession  ...                     [Romance, Shoujo]\n",
              "4                                                                                                                                                                               girl wealthy family accidentally lost virginity . get insulted party waking next morning . swore get revenge got closer instead  ...                      [Drama, Romance]\n",
              "5                                                                             death criminal lived jail country palom entering mad game progress . battle freedom . meaning freedom . spill scum blood like butcher massacring people flavor freedom . swore use legend sword hand breaking apart shackle world  ...                     [Action, Fantasy]\n",
              "6   ippo ready face greatest test thus far boxer next opponent completely unlike anyone fought . cool calculating possessing intelligence rivaled ferocity brain ippo brawn . make matter worse old rival coach kamogawa past returned guide ippo genius opponent brings knowledge secret attack supposed un...  ...                               [Drama]\n",
              "7   lone mercenary traveling foreign land forced participate street brawl fighting back back gun toting man wearing pelt around shoulder . emerge attack unscathed man order become bodyguard . mercenary turned bodyguard learn surprising fact spirited fighter king near daily assassination attempt maje...  ...                              [Action]\n",
              "8                                                                             slapstick comedy three pretty girl struggling earn living pilot iron goblin delivery vessel . computer answer back space pirate tail romantic entanglement client cause friction trio . final episode throw alien eterna race mix  ...                              [Comedy]\n",
              "9                                                              unfair heaven strange world wing forged flight god forsaken duck chicken ostrich . teasing dream soaring sky confining earth . world everyone born god blessing anomaly . ergera outclass god abandoned whilst simultaneously saving people love  ...          [Adventure, Comedy, Fantasy]\n",
              "10                                                                                                                                                                                                                                                  got see neighbor inside life chance . happening . take mind  ...                             [Romance]\n",
              "11                                                                        raven start sing people die unstable world weak government mafia escaping sin . order reduce unpunished crime committed mafia strength born agent merciless kill withought hesitation . last enforcement organization known red raven  ...                              [Action]\n",
              "12                                                       unusual restaurant located suburb japan invite take look livelihood staff . appeal sens mention cookware personified . pandra restaurant begin cutting board journey find place restaurant easy plastic cutting board amongst experienced utensil make  ...                              [Comedy]\n",
              "13  man call hakase professor work top class . engineering . wish create perfect android want use beautiful male model hinatsu base . hakase becomes bit obsessed real hinatsu investigation . hakase perverted many strange thing learn anything everything hinatsu . hinatsu want nothing nut job wish loo...  ...                                [Yaoi]\n",
              "14                                                                   sakurai new hire secret relationship bos chief akimoto . even though chief gentle suave unexpectedly pro come department still old man sakurai eye . however akimoto see right rather naive sakurai spoil point keep demanding come regret  ...                                [Yaoi]\n",
              "15                                                                                                                                   valt aoi total newbie come beyblade battle one beat love game . valt set sight district tournament held hometown . first master move technique show . natural talent shine  ...  [Action, Adventure, Comedy, Shounen]\n",
              "16                                                          year num tide zombie outbreak exploded everyone homeland humanity launch large scale battle endless night awaiting arrival dawn . end approaching going way . faith shattered piece buried ash raging fire . within blazing fire newborn hope arise  ...                              [Action]\n",
              "17  whether tongue body use . long man fell love . back old alley yoruji always looking night sky . finally opened ramen shop yogoto despite name far cry away beloved star . however one late night yoruji discovers akemi posse special tongue . two begin work together order make best tasting ramen . a...  ...                                [Yaoi]\n",
              "18  second son family beauty naruse shouta world famous hairstylist model . constantly shower affection ichii hisashi grim pure hearted high school student life across street . due self imposed num prohibited rule shouta impatiently waiting hisashi grow . however none soon hisashi num birthday approach  ...                         [Drama, Yaoi]\n",
              "19  deep sandy plain middle eastern kingdom asran japanese photojournalist makoto shinjou travel remote airbase area num document activity mercenary destroy country enemy living . among rank shin kazama japanese ace pilot tricked former best friend signing contract asran fighter squad . lost career ...  ...                       [Action, Drama]\n",
              "\n",
              "[20 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAZsORBO4RTE"
      },
      "source": [
        "numpy_array = comparisons_df.to_numpy()\n",
        "np.savetxt(\"comparisons_df.txt\", numpy_array, fmt = \"%s\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UW0E-i6m43Gz"
      },
      "source": [
        "!cp /content/comparisons.csv /content/drive/MyDrive/Final_NLP_Project/Part5/bert_classification_lm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWFd18u3zlF8"
      },
      "source": [
        "## Optimizing threshold value for micro F1 score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6mDy1lw0S4y"
      },
      "source": [
        "Doing this may result in a trade offs between precision, flat accuracy and micro F1 accuracy. You may tune the threshold however you want."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHlhb2lvar8V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef2b43b5-b77f-46f7-97ad-f3831aaabd40"
      },
      "source": [
        "# Calculate Accuracy - maximize F1 accuracy by tuning threshold values. First with 'macro_thresholds' on the order of e^-1 then with 'micro_thresholds' on the order of e^-2\n",
        "\n",
        "macro_thresholds = np.array(range(1,10))/10\n",
        "\n",
        "f1_results, flat_acc_results = [], []\n",
        "for th in macro_thresholds:\n",
        "  pred_bools = [pl>th for pl in pred_labels]\n",
        "  test_f1_accuracy = f1_score(true_bools,pred_bools,average='micro')\n",
        "  test_flat_accuracy = accuracy_score(true_bools, pred_bools)\n",
        "  f1_results.append(test_f1_accuracy)\n",
        "  flat_acc_results.append(test_flat_accuracy)\n",
        "\n",
        "best_macro_th = macro_thresholds[np.argmax(f1_results)] #best macro threshold value\n",
        "\n",
        "micro_thresholds = (np.array(range(10))/100)+best_macro_th #calculating micro threshold values\n",
        "\n",
        "f1_results, flat_acc_results = [], []\n",
        "for th in micro_thresholds:\n",
        "  pred_bools = [pl>th for pl in pred_labels]\n",
        "  test_f1_accuracy = f1_score(true_bools,pred_bools,average='micro')\n",
        "  test_flat_accuracy = accuracy_score(true_bools, pred_bools)\n",
        "  f1_results.append(test_f1_accuracy)\n",
        "  flat_acc_results.append(test_flat_accuracy)\n",
        "\n",
        "best_f1_idx = np.argmax(f1_results) #best threshold value\n",
        "\n",
        "# Printing and saving classification report\n",
        "print('Best Threshold: ', micro_thresholds[best_f1_idx])\n",
        "print('Test F1 Accuracy: ', f1_results[best_f1_idx])\n",
        "print('Test Flat Accuracy: ', flat_acc_results[best_f1_idx], '\\n')\n",
        "\n",
        "best_pred_bools = [pl>micro_thresholds[best_f1_idx] for pl in pred_labels]\n",
        "clf_report_optimized = classification_report(true_bools,best_pred_bools, target_names=label_cols)\n",
        "pickle.dump(clf_report_optimized, open('classification_report_optimized.txt','wb'))\n",
        "print(clf_report_optimized)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best Threshold:  0.3\n",
            "Test F1 Accuracy:  0.6308065932930295\n",
            "Test Flat Accuracy:  0.15616512764801738 \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Action       0.61      0.77      0.68       873\n",
            "   Adventure       0.55      0.63      0.59       462\n",
            "      Comedy       0.55      0.70      0.62      1145\n",
            "       Drama       0.47      0.68      0.55      1111\n",
            "     Fantasy       0.66      0.74      0.70       819\n",
            "     Romance       0.66      0.85      0.74      1560\n",
            " School Life       0.66      0.61      0.64       657\n",
            "      Sci Fi       0.56      0.57      0.57       312\n",
            "      Shoujo       0.50      0.57      0.53       476\n",
            "     Shounen       0.50      0.40      0.44       489\n",
            "Supernatural       0.57      0.50      0.53       468\n",
            "        Yaoi       0.63      0.75      0.68       505\n",
            "\n",
            "   micro avg       0.58      0.69      0.63      8877\n",
            "   macro avg       0.58      0.65      0.61      8877\n",
            "weighted avg       0.58      0.69      0.63      8877\n",
            " samples avg       0.60      0.69      0.60      8877\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBg6UCYAYtIe"
      },
      "source": [
        "!cp /content/classification_report_optimized.txt /content/drive/MyDrive/Final_NLP_Project/Part5/bert_classification_lm"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}